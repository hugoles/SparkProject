# Apache Spark for ETL and Data Warehouse Construction

[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

## Project Summary

The project utilizes Apache Spark in the Google Colab environment to perform the ETL (Extraction, Transformation, and Loading) process of data, with the ultimate goal of building a robust Data Warehouse in Hive. Apache Spark is a distributed data processing tool that offers efficiency and scalability, making it ideal for handling large datasets.

## Features

- **ETL with Apache Spark:** Harness the power of Apache Spark to efficiently and distributedly extract, transform, and load data (Use both `pyspark.pandas` and `pyspark.sql` for enhanced data processing capabilities).
  
- **Google Colab Environment(How to use):** 
   - Access [Google Colab](https://colab.research.google.com/).
   - Import the notebook `PysparkProject.ipynb`.
   - Execute the notebook cells as needed, following the provided instructions.

- **Data Warehouse Construction:** Build a robust Data Warehouse that meets the storage and data analysis needs.

- **Hive Tables:** Utilize Hive to load and save the data. The integration of Hive with the Apache Spark environment provides several advantages for this project like Schema-on-Read, SQL-Like Queries, Metadata Management, Scalability e Data Warehousing
- 
